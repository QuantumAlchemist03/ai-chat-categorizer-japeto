{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610cda66-fb3d-4401-8629-beeb7f4fbf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from nlpaug) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\alif\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alif\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alif\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alif\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\alif\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alif\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown, nlpaug\n",
      "Successfully installed gdown-5.2.0 nlpaug-1.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c70133-be9b-46aa-80ad-be2e2d913acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import nlpaug.augmenter.word as naw\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c575ac-43ad-4ef5-961d-91cb3059949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "# Assuming the dataset is in the same directory as your notebook\n",
    "df = pd.read_excel('chatbot_5class_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4b90a9-7fb8-41bf-a509-5f7fb284a37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                                text merged_category\n",
      "0  can you make an llm to talk to my cat tibbles ...        Services\n",
      "1  i am looking to integrate the openai api into ...        Services\n",
      "2  do you build intelligent automation tools yes ...        Services\n",
      "3  im concerend about ai dont worry ai is here to...        Services\n",
      "4  pouvezvous intgrer lintelligence artificielle ...        Services\n",
      "\n",
      "Class Distribution:\n",
      "merged_category\n",
      "General       207\n",
      "Info          152\n",
      "Support        80\n",
      "Services       76\n",
      "Engagement     53\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows and class distribution\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['merged_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fb64f7-2530-435e-b750-36a2de7ae0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing and Augmentation\n",
    "# We'll augment the data to balance the classes by generating paraphrased examples for underrepresented categories\n",
    "\n",
    "# Initialize the SBERT model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to augment text data using synonym replacement\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_text(text, num_augmentations=2):\n",
    "    augmented_texts = [aug.augment(text) for _ in range(num_augmentations)]\n",
    "    return [text] + [t[0] for t in augmented_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d18e6b7-e5cf-4828-b732-70d83fc005f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Augmentation:\n",
      "merged_category\n",
      "General       207\n",
      "Info          152\n",
      "Support        80\n",
      "Services       76\n",
      "Engagement     53\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Augment underrepresented classes (e.g., those with fewer than 40 samples)\n",
    "min_samples = 40\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for category in df['merged_category'].unique():\n",
    "    category_df = df[df['merged_category'] == category]\n",
    "    num_samples = len(category_df)\n",
    "    \n",
    "    if num_samples < min_samples:\n",
    "        # Calculate how many new samples we need\n",
    "        num_to_augment = min_samples - num_samples\n",
    "        # Select texts to augment\n",
    "        texts_to_augment = category_df['text'].sample(n=min(num_samples, num_to_augment), replace=True)\n",
    "        for text in texts_to_augment:\n",
    "            new_texts = augment_text(text, num_augmentations=2)\n",
    "            augmented_texts.extend(new_texts)\n",
    "            augmented_labels.extend([category] * len(new_texts))\n",
    "    else:\n",
    "        augmented_texts.extend(category_df['text'].tolist())\n",
    "        augmented_labels.extend(category_df['merged_category'].tolist())\n",
    "\n",
    "# Create a new DataFrame with augmented data\n",
    "augmented_df = pd.DataFrame({'text': augmented_texts, 'merged_category': augmented_labels})\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"\\nClass Distribution After Augmentation:\")\n",
    "print(augmented_df['merged_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3fd4253-53a3-4cdb-9fc2-576a0d82fd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66becba24f6b439e9c1a3dac11450152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Feature Engineering with SBERT\n",
    "# Encode the text data into embeddings\n",
    "embeddings = model.encode(augmented_df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(augmented_df['merged_category'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bbfa57-3c2c-4b8d-9aef-1c9921911855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train-Test Split\n",
    "# Use a 90%-10% split to give the model more training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f43d199-1bd8-4281-800a-e1cbd12d4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale'}\n",
      "Best Cross-Validation Accuracy: 0.7280982295830954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Model Training with SVM\n",
    "# Use SVM with RBF kernel instead of Logistic Regression\n",
    "svm = SVC(kernel='rbf', probability=True, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec407fc-20b9-4964-b885-d5529c03ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Accuracy: 0.7192982456140351\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Engagement       0.67      0.40      0.50         5\n",
      "     General       0.81      0.81      0.81        21\n",
      "        Info       0.72      0.87      0.79        15\n",
      "    Services       0.56      0.62      0.59         8\n",
      "     Support       0.67      0.50      0.57         8\n",
      "\n",
      "    accuracy                           0.72        57\n",
      "   macro avg       0.68      0.64      0.65        57\n",
      "weighted avg       0.72      0.72      0.71        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluation on Test Set\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest Set Accuracy:\", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ce4d60-9b4c-4246-a9e8-b13cee19a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5-Fold Cross-Validation Scores: [0.57894737 0.69298246 0.74561404 0.76106195 0.54867257]\n",
      "Mean CV Accuracy: 0.6654556745846918\n",
      "Standard Deviation: 0.08653973326406365\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Cross-Validation for Robustness\n",
    "# Perform 5-fold cross-validation on the entire dataset\n",
    "cv_scores = cross_val_score(best_model, embeddings, y, cv=5, scoring='accuracy')\n",
    "print(\"\\n5-Fold Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation:\", cv_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3413716-e43c-49f3-bfa5-3cfe62d9683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Uncertain Predictions (confidence < 60%): 17\n",
      "Uncertain Predictions (indices): [ 5 12 14 17 19 20 21 22 24 33 37 39 40 48 49 50 56]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Handling Uncertainty\n",
    "# Flag predictions with low confidence for manual review\n",
    "proba = best_model.predict_proba(X_test)\n",
    "max_proba = np.max(proba, axis=1)\n",
    "uncertain_indices = np.where(max_proba < 0.6)[0]\n",
    "print(\"\\nNumber of Uncertain Predictions (confidence < 60%):\", len(uncertain_indices))\n",
    "if len(uncertain_indices) > 0:\n",
    "    print(\"Uncertain Predictions (indices):\", uncertain_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ebced2-db4a-426f-9344-e2145c863901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model, label encoder, and SBERT model saved for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save the Model (Optional)\n",
    "# You can save the model for deployment\n",
    "import joblib\n",
    "joblib.dump(best_model, 'svm_chatbot_classifier.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "joblib.dump(model, 'sbert_model.pkl')\n",
    "\n",
    "print(\"\\nModel, label encoder, and SBERT model saved for deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457ce99-72da-44d6-8f29-b302f5556000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
